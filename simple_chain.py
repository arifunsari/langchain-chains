from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

prompt = PromptTemplate(
    template='Generate 5 interesting facts about {topic}',
    input_variables=['topic']
)

model = ChatOpenAI()

parser = StrOutputParser()

chain = prompt | model | parser

result = chain.invoke({'topic':'cricket'})

print(result)

chain.get_graph().print_ascii()


# from langchain_openai import ChatOpenAI  # Importing the ChatOpenAI LLM from langchain_openai integration
# from dotenv import load_dotenv  # To load environment variables (like API keys) from a .env file
# from langchain_core.prompts import PromptTemplate  # Import PromptTemplate for creating prompt templates
# from langchain_core.output_parsers import StrOutputParser  # Import parser that returns raw string output

# load_dotenv()  # Load environment variables from .env file into the environment (e.g., for OpenAI API key)

# # Create a prompt template that will ask the model to generate 5 interesting facts about a given topic
# prompt = PromptTemplate(
#     template='Generate 5 interesting facts about {topic}',  # Template string with a placeholder for 'topic'
#     input_variables=['topic']  # Specify that 'topic' is the input variable to be filled in the prompt
# )

# model = ChatOpenAI()  # Initialize the ChatOpenAI language model with default settings (e.g., gpt-3.5 or gpt-4)

# parser = StrOutputParser()  # Initialize a parser that will parse the model's response as a plain string

# # Create a chain by piping the prompt -> model -> parser (LangChain composability using "|")
# chain = prompt | model | parser

# # Invoke the chain by passing in a topic; this sends the final prompt to the model and gets the parsed result
# result = chain.invoke({'topic': 'cricket'})  # The placeholder {topic} will be replaced with "cricket"

# print(result)  # Print the output generated by the model (5 interesting facts about cricket)

# chain.get_graph().print_ascii()  # Visualize the chain's structure in ASCII format (prompt -> model -> parser)
